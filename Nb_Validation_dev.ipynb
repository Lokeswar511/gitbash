{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f6b04f-e11e-4a91-97b3-282e7db8e1f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffce96d3-a586-490f-9904-bb8a772dc97b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: [FileInfo(path='dbfs:/FileStore/', name='FileStore/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/Orders/', name='Orders/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/People/', name='People/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/Returns/', name='Returns/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/databricks-datasets/', name='databricks-datasets/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/databricks-results/', name='databricks-results/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/mnt/', name='mnt/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/project/', name='project/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/stage.stage_store/', name='stage.stage_store/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/user/', name='user/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "%run /validation/SetUpBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20d1feb-e48b-4dd8-807e-dd37b9af039f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reference_df = spark.read.format('csv').option(\"header\", True).option(\"inferSchema\", True).load(reference_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3012adb9-4167-4423-8a39-39e5ef20e17f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-------------+-----------+-------------+-------------+----------+----------+\n| Id|SrcContainerName|SrcFolderName|SrcFileName|   SrcColumns|SrcColumnType| CreatedAt|updated_at|\n+---+----------------+-------------+-----------+-------------+-------------+----------+----------+\n|  1|      xyenta-stg|       Orders|Orders_data|       Row ID|      integer|08-12-2023|11-12-2023|\n|  2|      xyenta-stg|       Orders|Orders_data|     Order ID|       string|08-12-2023|11-12-2023|\n|  3|      xyenta-stg|       Orders|Orders_data|   Order Date|         date|08-12-2023|11-12-2023|\n|  4|      xyenta-stg|       Orders|Orders_data|    Ship Date|         date|08-12-2023|11-12-2023|\n|  5|      xyenta-stg|       Orders|Orders_data|    Ship Mode|       string|08-12-2023|11-12-2023|\n|  6|      xyenta-stg|       Orders|Orders_data|  Customer ID|       string|08-12-2023|11-12-2023|\n|  7|      xyenta-stg|       Orders|Orders_data|Customer Name|       string|08-12-2023|11-12-2023|\n|  8|      xyenta-stg|       Orders|Orders_data|      Segment|       string|08-12-2023|11-12-2023|\n|  9|      xyenta-stg|       Orders|Orders_data|         City|       string|08-12-2023|11-12-2023|\n| 10|      xyenta-stg|       Orders|Orders_data|        State|       string|08-12-2023|11-12-2023|\n+---+----------------+-------------+-----------+-------------+-------------+----------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "reference_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86d624e1-bfa6-4bf0-aa48-96d7893f0144",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(orders_path)\n",
    "\n",
    "returns_df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(returns_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77c7cfe3-4247-4712-8871-6a856d722729",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Row ID: integer (nullable = true)\n |-- Order ID: string (nullable = true)\n |-- Order Date: date (nullable = true)\n |-- Ship Date: date (nullable = true)\n |-- Ship Mode: string (nullable = true)\n |-- Customer ID: string (nullable = true)\n |-- Customer Name: string (nullable = true)\n |-- Segment: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Country: string (nullable = true)\n |-- Postal Code: integer (nullable = true)\n |-- Market: string (nullable = true)\n |-- Region: string (nullable = true)\n |-- Product ID: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Sub-Category: string (nullable = true)\n |-- Product Name: string (nullable = true)\n |-- Sales: double (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Discount: double (nullable = true)\n |-- Profit: double (nullable = true)\n |-- Shipping Cost: double (nullable = true)\n |-- Order Priority: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59b7fc2-f8c4-4c19-9577-ac2d6ba39ed4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#columns comparision \n",
    "\n",
    "def column_comparision(df, filename, ref_df):\n",
    "    \"\"\"\n",
    "        df : pass the dataframe to which we need to validate\n",
    "        filename : pass this parameter to get the required/related information about particular file from ref_df\n",
    "        ref_df : pass the reference_df \n",
    "    \"\"\"\n",
    "    #get the columns from both df and ref_df\n",
    "    df_columns = set(df.columns)\n",
    "\n",
    "    ref_filtered = ref_df.filter(col('SrcFileName') == filename)\n",
    "\n",
    "    ref_columns = set(ref_filtered.select(\"SrcColumns\").rdd.flatMap(lambda x:x).collect())\n",
    "\n",
    "    #check for missing columns\n",
    "    missing_columns = ref_columns - df_columns\n",
    "\n",
    "    if missing_columns :\n",
    "        print(f\"Column names do not match. Missing columns: {missing_columns}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"columns match for {filename}.\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1798e18e-9a6f-4b59-adb6-ee5313f4ec99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns match for Orders_data.\ncolumns match for Returns_data.\nOut[19]: True"
     ]
    }
   ],
   "source": [
    "#for orders_data\n",
    "column_comparision(df=orders_df, filename='Orders_data', ref_df=reference_df)\n",
    "\n",
    "#for returns_data\n",
    "column_comparision(df= returns_df, filename='Returns_data', ref_df= reference_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d5d538-9783-45e5-b66c-1a60a4dd7bca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Schema comparision\n",
    "def schema_comparision(df, filename, ref_df):\n",
    "    #filtered accoridng to filename and iterate through list and get the SrcColumns and SrcColumnTypes\n",
    "    ref_schema_filtered = ref_df.filter(col(\"SrcFileName\") == filename)\n",
    "\n",
    "    for x in ref_schema_filtered.collect():\n",
    "\n",
    "        columnNames = x['SrcColumns']\n",
    "        #converting str to list to avoid getting the only characters from list at time of returning or printing missied columns\n",
    "        columnNamesList = [x.strip() for x in columnNames.split(\",\")]\n",
    "\n",
    "        refDataTypes = x['SrcColumnType'] \n",
    "\n",
    "        refTypeList = [x.strip() for x in refDataTypes.split(\",\")]\n",
    "        #get the schema of the dataframe to which we need to target\n",
    "        # dataFrameTypes = df.schema[columnNames].dataType.simpleString()\n",
    "        dataFrameTypes = [field.dataType.simpleString() for field in df.schema.fields]\n",
    "        # print(dataFrameTypes2)\n",
    "\n",
    "        missmatchedcolumns = [[col_names, df_types, ref_types] for col_names, df_types, ref_types in zip(columnNamesList, dataFrameTypes,refTypeList) if df_types != ref_types]\n",
    "\n",
    "        # print(missmatchedcolumns)\n",
    "\n",
    "        if missmatchedcolumns:\n",
    "\n",
    "            print(\"Schema mismatch for the following columns:\")\n",
    "            for col_name, df_type, ref_type in missmatchedcolumns:\n",
    "\n",
    "                print(f\"Column: {col_name}, DataFrame Type: {df_type}, Reference Type: {ref_type}\")\n",
    "            return False\n",
    "       \n",
    "    print(\"All columns schema matched\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89488903-83a8-4474-af4b-0a6076841827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns schema matched\nOut[31]: True"
     ]
    }
   ],
   "source": [
    "#for Returns_data\n",
    "schema_comparision(df= returns_df, filename='Returns_data', ref_df= reference_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Nb_Validation_dev",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
