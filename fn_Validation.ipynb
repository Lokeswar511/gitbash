{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f6b04f-e11e-4a91-97b3-282e7db8e1f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffce96d3-a586-490f-9904-bb8a772dc97b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: [FileInfo(path='dbfs:/FileStore/', name='FileStore/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/Orders/', name='Orders/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/People/', name='People/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/Returns/', name='Returns/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/databricks-datasets/', name='databricks-datasets/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/databricks-results/', name='databricks-results/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/mnt/', name='mnt/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/project/', name='project/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/stage.stage_store/', name='stage.stage_store/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/user/', name='user/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "%run /validation/SetUpBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20d1feb-e48b-4dd8-807e-dd37b9af039f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reference_df = spark.read.format('csv').option(\"header\", True).option(\"inferSchema\", True).load(reference_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3012adb9-4167-4423-8a39-39e5ef20e17f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+--------------+-------------+----------+----------+\n| Id|SrcFolderName| SrcFileName|    SrcColumns|SrcColumnType| CreatedAt|updated_at|\n+---+-------------+------------+--------------+-------------+----------+----------+\n|  1|       Orders| Orders_data|        Row ID|          int|08-12-2023|11-12-2023|\n|  2|       Orders| Orders_data|      Order ID|       string|08-12-2023|11-12-2023|\n|  3|       Orders| Orders_data|    Order Date|         date|08-12-2023|11-12-2023|\n|  4|       Orders| Orders_data|     Ship Date|         date|08-12-2023|11-12-2023|\n|  5|       Orders| Orders_data|     Ship Mode|       string|08-12-2023|11-12-2023|\n|  6|       Orders| Orders_data|   Customer ID|       string|08-12-2023|11-12-2023|\n|  7|       Orders| Orders_data| Customer Name|       string|08-12-2023|11-12-2023|\n|  8|       Orders| Orders_data|       Segment|       string|08-12-2023|11-12-2023|\n|  9|       Orders| Orders_data|          City|       string|08-12-2023|11-12-2023|\n| 10|       Orders| Orders_data|         State|       string|08-12-2023|11-12-2023|\n| 11|       Orders| Orders_data|       Country|       string|08-12-2023|11-12-2023|\n| 12|       Orders| Orders_data|   Postal Code|          int|08-12-2023|11-12-2023|\n| 13|       Orders| Orders_data|        Market|       string|08-12-2023|11-12-2023|\n| 14|       Orders| Orders_data|        Region|       string|08-12-2023|11-12-2023|\n| 15|       Orders| Orders_data|    Product ID|       string|08-12-2023|11-12-2023|\n| 16|       Orders| Orders_data|      Category|       string|08-12-2023|11-12-2023|\n| 17|       Orders| Orders_data|  Sub-Category|       string|08-12-2023|11-12-2023|\n| 18|       Orders| Orders_data|  Product Name|       string|08-12-2023|11-12-2023|\n| 19|       Orders| Orders_data|         Sales|       double|08-12-2023|11-12-2023|\n| 20|       Orders| Orders_data|      Quantity|          int|08-12-2023|11-12-2023|\n| 21|       Orders| Orders_data|      Discount|       double|08-12-2023|11-12-2023|\n| 22|       Orders| Orders_data|        Profit|       double|08-12-2023|11-12-2023|\n| 23|       Orders| Orders_data| Shipping Cost|       double|08-12-2023|11-12-2023|\n| 24|       Orders| Orders_data|Order Priority|       string|08-12-2023|11-12-2023|\n| 25|      Returns|Returns_data|      Returned|       string|08-12-2023|11-12-2023|\n| 26|      Returns|Returns_data|      Order ID|       string|08-12-2023|11-12-2023|\n| 27|      Returns|Returns_data|        Market|       string|08-12-2023|11-12-2023|\n| 28|       People| People_data|        Person|       string|08-12-2023|11-12-2023|\n| 29|       People| People_data|        Region|       string|08-12-2023|11-12-2023|\n+---+-------------+------------+--------------+-------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "reference_df.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daa281f8-0fb4-4257-8da1-bada70c8cb0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86d624e1-bfa6-4bf0-aa48-96d7893f0144",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(orders_path)\n",
    "\n",
    "returns_df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(returns_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77c7cfe3-4247-4712-8871-6a856d722729",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Row ID: integer (nullable = true)\n |-- Order ID: string (nullable = true)\n |-- Order Date: date (nullable = true)\n |-- Ship Date: date (nullable = true)\n |-- Ship Mode: string (nullable = true)\n |-- Customer ID: string (nullable = true)\n |-- Customer Name: string (nullable = true)\n |-- Segment: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Country: string (nullable = true)\n |-- Postal Code: integer (nullable = true)\n |-- Market: string (nullable = true)\n |-- Region: string (nullable = true)\n |-- Product ID: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Sub-Category: string (nullable = true)\n |-- Product Name: string (nullable = true)\n |-- Sales: double (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Discount: double (nullable = true)\n |-- Profit: double (nullable = true)\n |-- Shipping Cost: double (nullable = true)\n |-- Order Priority: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65871b33-1d9a-406f-819b-15cbcf39c2b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[55]: StructType([StructField('Row ID', IntegerType(), True), StructField('Order ID', StringType(), True), StructField('Order Date', DateType(), True), StructField('Ship Date', DateType(), True), StructField('Ship Mode', StringType(), True), StructField('Customer ID', StringType(), True), StructField('Customer Name', StringType(), True), StructField('Segment', StringType(), True), StructField('City', StringType(), True), StructField('State', StringType(), True), StructField('Country', StringType(), True), StructField('Postal Code', IntegerType(), True), StructField('Market', StringType(), True), StructField('Region', StringType(), True), StructField('Product ID', StringType(), True), StructField('Category', StringType(), True), StructField('Sub-Category', StringType(), True), StructField('Product Name', StringType(), True), StructField('Sales', DoubleType(), True), StructField('Quantity', IntegerType(), True), StructField('Discount', DoubleType(), True), StructField('Profit', DoubleType(), True), StructField('Shipping Cost', DoubleType(), True), StructField('Order Priority', StringType(), True)])"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59b7fc2-f8c4-4c19-9577-ac2d6ba39ed4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#columns comparision \n",
    "def column_comparision(df, filename, ref_df):\n",
    "    \"\"\"\n",
    "        df : pass the dataframe to which we need to validate\n",
    "        filename : pass this parameter to get the required/related information about particular file from ref_df\n",
    "        ref_df : pass the reference_df \n",
    "    \"\"\"\n",
    "    try:\n",
    "        #get the columns from both df and ref_df and convert them to lower case\n",
    "        df_columns = set(map(str.lower,df.columns))\n",
    "        print(df_columns)\n",
    "        ref_filtered = ref_df.filter(col('SrcFileName') == filename)\n",
    "        ref_columns = set(map(str.lower, ref_filtered.select(\"SrcColumns\").rdd.flatMap(lambda x:x).collect()))\n",
    "        #check for missing columns\n",
    "        missing_columns = ref_columns - df_columns\n",
    "        if missing_columns :\n",
    "            print(f\"Column names do not match. Missing columns: {missing_columns}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"columns match for {filename}.\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1798e18e-9a6f-4b59-adb6-ee5313f4ec99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product name', 'market', 'city', 'order id', 'postal code', 'profit', 'sales', 'customer name', 'ship date', 'segment', 'state', 'quantity', 'order priority', 'row id', 'product id', 'sub-category', 'discount', 'ship mode', 'customer id', 'order date', 'region', 'shipping cost', 'category', 'country'}\ncolumns match for Orders_data.\nOut[23]: True"
     ]
    }
   ],
   "source": [
    "#for orders_data\n",
    "column_comparision(df=orders_df, filename='Orders_data', ref_df=reference_df)\n",
    "#for returns_data\n",
    "# column_comparision(df= returns_df, filename='Returns_data', ref_df= reference_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f46f9eaa-9a30-46c7-ac93-175f4e1b3df1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "a = \"Hello\"\n",
    "print(a.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d5d538-9783-45e5-b66c-1a60a4dd7bca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Schema comparision\n",
    "def schema_comparision(df, filename, ref_df):\n",
    "    try:\n",
    "        #filtered accoridng to filename and iterate through list and get the SrcColumns and SrcColumnTypes\n",
    "        ref_schema_filtered = ref_df.filter(col(\"SrcFileName\") == filename)\n",
    "\n",
    "        for x in ref_schema_filtered.collect():\n",
    "\n",
    "            columnNames = x['SrcColumns']\n",
    "\n",
    "            #converting str to list to avoid getting the only characters from list at time of returning or printing missied columns\n",
    "            columnNamesList = [x.strip().lower() for x in columnNames.split(\",\")]\n",
    "\n",
    "            refDataTypes = x['SrcColumnType'] \n",
    "\n",
    "            refTypeList = [x.strip().lower() for x in refDataTypes.split(\",\")]\n",
    "            #print(refDataTypes)\n",
    "            #get the schema of the dataframe to which we need to target\n",
    "            dataFrameTypes = df.schema[columnNames].dataType.simpleString()\n",
    "            #print(type(dataFrameTypes))\n",
    "            dataFrameTypesList = [x.strip().lower() for x in dataFrameTypes.split(\",\")]\n",
    "            #print(dataFrameTypesList)\n",
    "            # dataFrameTypes = [field.dataType.simpleString() for field in df.schema.fields]\n",
    "            # print(dataFrameTypes)\n",
    "\n",
    "            missmatchedcolumns = [[col_names, df_types, ref_types] for col_names, df_types, ref_types in zip(columnNamesList, dataFrameTypesList,refTypeList) if df_types != ref_types]\n",
    "\n",
    "            # print(missmatchedcolumns)\n",
    "            if missmatchedcolumns:\n",
    "                print(\"Schema mismatch for the following columns:\")\n",
    "                for col_name, df_type, ref_type in missmatchedcolumns:\n",
    "                    print(f\"ColumnName: {col_name}, DataFrameType: {df_type}, ReferenceType: {ref_type}\")\n",
    "                return False\n",
    "       \n",
    "        print(\"All columns schema matched\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89488903-83a8-4474-af4b-0a6076841827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns schema matched\nOut[26]: True"
     ]
    }
   ],
   "source": [
    "#for Returns_data\n",
    "schema_comparision(df= returns_df, filename='Returns_data', ref_df= reference_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f931c5c-97ce-4051-b8ad-851ce41b05f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns schema matched\nOut[76]: True"
     ]
    }
   ],
   "source": [
    "#for orders_data\n",
    "schema_comparision(df= orders_df, filename='Orders_data', ref_df= reference_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "fn_Validation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
